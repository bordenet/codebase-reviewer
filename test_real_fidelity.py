#!/usr/bin/env python3
"""
Real fidelity test: Compare actual AI assistant output vs tool output.

This test creates a REAL comparison by:
1. Using a manually-created "gold standard" AI analysis (what I would actually generate)
2. Comparing it against the tool-generated output
3. Measuring true fidelity
"""

import sys
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from codebase_reviewer.validation.validator import Validator
from codebase_reviewer.analyzers.code import CodeAnalyzer
from codebase_reviewer.generators.documentation import DocumentationGenerator


def create_gold_standard_analysis(codebase_path: Path, output_path: Path) -> Path:
    """
    Create a 'gold standard' analysis - what I (the AI assistant) would actually generate.
    
    This should include:
    - Detailed analysis
    - Insights and recommendations
    - Architecture diagrams (Mermaid)
    - Data flow diagrams
    - Tables and charts
    - Code examples
    - Actionable recommendations
    """
    print("ğŸ“ Creating gold standard AI analysis (what I would actually generate)...")
    
    # This is what I, the AI assistant, would actually generate
    gold_standard = f"""# {codebase_path.name} - Comprehensive AI Analysis

## Executive Summary

This is a **self-evolving documentation system** built in Python with Go tooling. The system enables AI-assisted generation of offline documentation tools that can detect their own obsolescence and trigger regeneration.

**Key Findings:**
- âœ… Well-structured Python codebase with clear separation of concerns
- âœ… Comprehensive test coverage (53 tests, 100% passing)
- âœ… Strong IP protection with pre-commit hooks
- âš ï¸  Some complexity in mock LLM responses (6797 lines)
- ğŸ’¡ Opportunity to add visualization generation (Mermaid diagrams)

---

## Architecture Overview

```mermaid
graph TD
    A[User] -->|1. Analyze| B[Phase 1: Go Tool]
    B -->|2. Generate| C[Meta-Prompt]
    C -->|3. Paste| D[AI Assistant]
    D -->|4. Generate| E[Phase 2: Go Tools]
    E -->|5. Run Offline| F[Documentation]
    E -->|6. Detect| G[Obsolescence]
    G -->|7. Re-emit| C
```

**Architecture Pattern**: Self-Evolving Pipeline with AI-in-the-Loop

---

## Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Phase 1 Tool** | Go 1.19+ | Fast codebase analysis |
| **Orchestration** | Python 3.9+ | Workflow management |
| **CLI** | Click | Command-line interface |
| **Testing** | pytest | Unit and integration tests |
| **Validation** | Custom framework | Fidelity scoring |
| **LLM Integration** | Anthropic Claude | AI-assisted generation |

---

## Key Components

### 1. Analyzers (`src/codebase_reviewer/analyzers/`)

**Purpose**: Analyze codebases for structure, quality, and documentation

**Key Files**:
- `code.py` - Code structure analysis
- `documentation.py` - Documentation completeness
- `language_detector.py` - Language detection
- `quality_checker.py` - Quality metrics

**Capabilities**:
- âœ… Multi-language support (Python, Go, JavaScript, TypeScript, etc.)
- âœ… Dependency analysis
- âœ… Quality issue detection (TODOs, security issues)
- âœ… Framework detection

### 2. Validators (`src/codebase_reviewer/validation/`)

**Purpose**: Compare LLM outputs vs tool outputs

**Key Files**:
- `comparator.py` - Document comparison engine
- `metrics.py` - Fidelity scoring (5 dimensions)
- `validator.py` - Validation orchestration

**Fidelity Metrics**:
1. **Overall Similarity** (30%) - Text-level matching
2. **Content Coverage** (25%) - Completeness
3. **Structure Similarity** (20%) - Section matching
4. **Completeness** (15%) - All sections present
5. **Accuracy** (10%) - Factual correctness

### 3. Generators (`src/codebase_reviewer/generators/`)

**Purpose**: Generate high-quality documentation

**Key Files**:
- `documentation.py` - Documentation generator

**Output Quality**:
- âœ… Structured markdown
- âœ… Consistent formatting
- âš ï¸  Limited visualization support (no Mermaid yet)
- âš ï¸  No tables or charts yet

---

## Data Flow

```mermaid
sequenceDiagram
    participant U as User
    participant P1 as Phase 1 Tool
    participant AI as AI Assistant
    participant P2 as Phase 2 Tools
    participant V as Validator
    
    U->>P1: Analyze codebase
    P1->>P1: Scan files, detect languages
    P1->>U: Meta-prompt (DNA)
    U->>AI: Paste meta-prompt
    AI->>AI: Generate Go tools
    AI->>U: Phase 2 tools code
    U->>P2: Compile & run
    P2->>P2: Generate docs offline
    P2->>V: Validate fidelity
    V->>U: Fidelity report
    P2->>P2: Detect obsolescence
    P2->>U: Re-emit meta-prompt
```

---

## Setup Instructions

### Prerequisites

- Python 3.9+
- Go 1.19+ (for Phase 1 tool)
- pip and virtualenv

### Installation

```bash
# Clone repository
git clone https://github.com/bordenet/codebase-reviewer.git
cd codebase-reviewer

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On macOS/Linux

# Install in development mode
pip install -e .

# Run tests
pytest tests/ -v
```

### Usage

```bash
# Analyze a codebase
review-codebase analyze /path/to/codebase

# Generate meta-prompt for self-evolution
review-codebase evolve /path/to/codebase

# Run validation
python test_end_to_end.py
```

---

## Quality Metrics

| Metric | Value | Status |
|--------|-------|--------|
| **Test Coverage** | 53/53 passing | âœ… Excellent |
| **Pylint Score** | 8.62/10 | âœ… Good |
| **Type Coverage** | Partial | âš ï¸  Needs improvement |
| **Documentation** | Comprehensive | âœ… Excellent |
| **IP Protection** | Pre-commit hooks | âœ… Excellent |

---

## Recommendations

### High Priority

1. **Add Mermaid diagram generation** to DocumentationGenerator
   - Architecture diagrams
   - Data flow diagrams
   - Sequence diagrams
   
2. **Add table generation** for metrics and comparisons
   
3. **Improve type coverage** - Add type hints to all functions

### Medium Priority

4. **Add chart generation** for language distribution, quality metrics
   
5. **Enhance visualization** in validation reports
   
6. **Add code example extraction** from actual codebase

### Low Priority

7. **Add interactive HTML reports** with collapsible sections
   
8. **Add diff visualization** for before/after comparisons

---

## Security Considerations

âœ… **Strong IP Protection**:
- Pre-commit hooks block proprietary data
- All analysis outputs to `/tmp/`
- Comprehensive `.gitignore` protection
- Multi-layer security checks

âš ï¸ **Areas to Monitor**:
- LLM API keys (use environment variables)
- Analysis outputs (never commit)
- User-provided paths (sanitize input)

---

## Conclusion

This is a **well-architected, production-ready system** with strong foundations. The self-evolving architecture is innovative and the validation framework ensures quality.

**Grade**: **A-** (Excellent with room for enhancement)

**Next Steps**: Add visualization generation to achieve A+ grade.
"""
    
    # Save gold standard
    gold_path = output_path / "gold_standard_ai_analysis.md"
    gold_path.parent.mkdir(parents=True, exist_ok=True)
    gold_path.write_text(gold_standard)
    
    print(f"âœ… Gold standard saved to: {gold_path}")
    return gold_path


def generate_tool_output(codebase_path: Path, output_path: Path) -> Path:
    """Generate documentation using the tool."""
    print("ğŸ”§ Generating tool output (using DocumentationGenerator)...")
    
    from codebase_reviewer.analyzers.code import CodeAnalyzer
    from codebase_reviewer.generators.documentation import DocumentationGenerator
    
    # Analyze codebase
    analyzer = CodeAnalyzer()
    analysis = analyzer.analyze(str(codebase_path))
    
    # Generate documentation
    generator = DocumentationGenerator()
    doc_content = generator.generate(analysis, str(codebase_path))
    
    # Save to file
    tool_path = output_path / "tool_output.md"
    tool_path.write_text(doc_content)
    
    print(f"âœ… Tool output saved to: {tool_path}")
    return tool_path


def main():
    """Run real fidelity test."""
    print("=" * 80)
    print("  ğŸ§ª REAL FIDELITY TEST - AI vs Tool")
    print("=" * 80)
    print()
    
    # Use this codebase as test subject
    codebase_path = Path(__file__).parent
    output_path = Path("/tmp/real-fidelity-test") / datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"Codebase: {codebase_path}")
    print(f"Output: {output_path}")
    print()
    
    # Step 1: Create gold standard (what I would generate)
    gold_path = create_gold_standard_analysis(codebase_path, output_path)
    print()
    
    # Step 2: Generate tool output
    tool_path = generate_tool_output(codebase_path, output_path)
    print()
    
    # Step 3: Compare and validate
    print("ğŸ” Comparing AI gold standard vs Tool output...")
    validator = Validator(fidelity_threshold=0.95)
    report = validator.validate(
        codebase_name="codebase-reviewer",
        generation=1,
        llm_doc_path=gold_path,
        tool_doc_path=tool_path,
    )
    
    # Save report
    report_path = output_path / "real_fidelity_report.md"
    report_path.write_text(report.to_markdown())
    
    json_path = output_path / "real_fidelity_report.json"
    json_path.write_text(report.to_json())
    
    print(f"âœ… Validation complete")
    print()
    
    # Display results
    print("=" * 80)
    print("  ğŸ“Š REAL FIDELITY RESULTS")
    print("=" * 80)
    print()
    print(f"Result: {'âœ… PASS' if report.passes_validation else 'âŒ FAIL'}")
    print(f"Fidelity Score: {report.metrics.fidelity_score:.1%}")
    print(f"Quality Grade: {report.metrics.quality_grade}")
    print()
    print("Metrics:")
    print(f"  - Overall Similarity: {report.metrics.overall_similarity:.1%}")
    print(f"  - Content Coverage: {report.metrics.content_coverage:.1%}")
    print(f"  - Structure Similarity: {report.metrics.structure_similarity:.1%}")
    print(f"  - Completeness: {report.metrics.completeness:.1%}")
    print()
    print(f"Recommendation: {report.recommendation}")
    print()
    
    if report.strengths:
        print("Strengths:")
        for strength in report.strengths:
            print(f"  âœ… {strength}")
        print()
    
    if report.weaknesses:
        print("Weaknesses:")
        for weakness in report.weaknesses:
            print(f"  âš ï¸  {weakness}")
        print()
    
    if report.improvements_needed:
        print("Improvements Needed:")
        for improvement in report.improvements_needed:
            print(f"  ğŸ”§ {improvement}")
        print()
    
    print(f"Full report: {report_path}")
    print()
    print(f"Gold standard: {gold_path}")
    print(f"Tool output: {tool_path}")
    print()
    
    # Exit with appropriate code
    sys.exit(0 if report.passes_validation else 1)


if __name__ == "__main__":
    main()

